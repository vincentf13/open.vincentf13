# Open Exchange Core - 技術展示系列 Ep.1：系統架構與 SDK 設計

**影片長度：** 約 3.5 - 4 分鐘
**核心亮點：** CQRS (讀寫分離)、LMAX 架構、WAL、無狀態擴展、風控與結算一致性

---

## 1. 系統架構總覽 (System Architecture Overview)

**目標：** 深入剖析為何傳統架構無法支撐金融級高頻交易，並展示 Open Exchange Core 如何透過架構創新解決此問題。

| 時間   | 畫面 (Visual)                                                                                                                                                                                                     | 旁白腳本 (Audio)                                                                                                                                                                                                  | 執行建議 |
| :--- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--- |
| 0:00 | **[對比動畫：傳統 vs 現代]**<br>左邊顯示「傳統架構」：大量請求擠向一個 Database 圖示，DB 出現紅色鎖頭 (Lock)。<br>右邊顯示「Open Exchange Core」：數據像流水一樣通過管道 (Kafka)。                                                                                       | 傳統金融系統往往受限於資料庫的 ACID 鎖機制。當海量訂單湧入時，行級鎖會導致嚴重的資源競爭。Open Exchange Core 徹底摒棄了這種依賴，採用了**全異步的事件驅動架構**。                                                                                                               |      |
| 0:25 | **[架構特寫：LMAX 核心思想]**<br>畫面顯示一個類似 CPU 管道的圖示。<br>標註：**Disruptor Pattern / Ring Buffer**。<br>數據單向流動，無鎖競爭。                                                                                                          | 我們借鑒了 **LMAX Disruptor** 的架構思想。在最核心的撮合環節，我們完全移除了隨機磁碟 I/O 與鎖競爭，採用內存佇列進行定序。                                                                                                                                     |      |
| 0:40 | **[深度解析：WAL 與災難復原]**<br>畫面顯示一個 **File** 圖示，數據以 **Batch** 的形式快速寫入。<br>標註：**Sequential Write (順序寫)**。<br>然後演示系統崩潰 (Crash)，接著進度條快速跑動 (Replay)，內存狀態瞬間恢復。<br>右下角浮現未來規劃：**Raft Consensus / RingBuffer Optimization**。 | 為了確保數據絕對安全，我們實現了 **WAL (Write-Ahead Logging)** 機制。所有的撮合事件會先進行**批次順序寫入**——這也是整個撮合過程中**唯一的一次磁碟 I/O**。即使系統瞬間斷電，我們也能透過重放 WAL 日誌，在毫秒級內完全恢復內存狀態。<br>此外，我們規劃了基於 **Raft 協議** 的事件複製與 **RingBuffer** 優化，進一步提升事件層級的高可用性。 |      |

---

## 2. 領域服務架構特寫 (Domain Service Architecture)

**目標：** 針對不同服務屬性 (寫入密集、查詢密集、計算密集) 展示差異化的架構設計。

| 時間   | 畫面 (Visual)                                                                                                                                        | 旁白腳本 (Audio)                                                                                                                                                  | 執行建議                      |
| :--- | :------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------ |
| 1:15 | **[CQRS 宏觀視角]**<br>畫面將服務分為兩群：<br>⬅️ **Command Side (寫):** Order, Matching (紅色)<br>➡️ **Query Side (讀):** Market, History (藍色)                      | 整個系統遵循 **CQRS (命令查詢職責分離)** 原則。針對不同的業務型態，我們採用了截然不同的優化策略。                                                                                                       | 引入 CQRS 概念。               |
| 1:25 | **[Matching & Order (寫入極致)]**<br>特寫撮合引擎。<br>關鍵字：**Batching**, **Single-Thread**, **No-Lock**。                                                      | 對於寫入密集的 **Matching Service**，我們追求極致的低延遲。透過批次處理與單執行緒無鎖設計，我們將硬體性能壓榨到了極限，確保每一筆訂單都能在微秒級完成定序與撮合。                                                                   | 總結寫入端特點。                  |
| 1:40 | **[Market Data (查詢與推播)]**<br>特寫行情服務。<br>畫面顯示大量 WebSocket 連線 (用戶端)。<br>後端連接著 Redis Cluster。<br>關鍵字：**Netty Epoll**, **Zero-Copy**, **Redis Cache**。 | 對於查詢密集的 **Market Data Service**，重點則在於高併發的讀取與推播。我們利用 **Netty 的 Epoll 機制** 維護海量 WebSocket 連線，並結合 Redis 實現熱點數據的毫秒級推播，確保百萬用戶能同時看到最新的 K 線與深度圖。                     | 強調 Netty 與 Redis 在讀取端的價值。 |
| 1:55 | **[Risk & Asset (風控與結算)]**<br>特寫風控與資產模組。<br>Risk 顯示：**Pre-Trade Check (事前風控)**。<br>Asset 顯示：**Double-Entry (複式記帳)**。<br>兩者中間有強一致性的連線。              | 而對於最敏感的 **Risk (風控)** 與 **Asset (結算)**，架構的核心是「正確性」。風控採用事前檢查模型 (Pre-Trade Check) 實時計算保證金；結算層則嚴格遵循**複式記帳法 (Double-Entry Bookkeeping)**，確保每一分錢的流動都有借貸平衡，實現資金零誤差。 | 點出金融系統的核心：正確性與複式記帳。       |
| 2:15 | **[彈性擴展總結]**<br>畫面縮小回全景。<br>Gateway, Order, Market 像細胞分裂一樣快速複製 (Stateless)。<br>Matching, Asset 則依據幣種分片 (Stateful)。                                 | 正因如此，我們的擴展策略也是分層的：Gateway 與 Market 等無狀態服務可以無限水平擴展以應對流量洪峰；而 Matching 與 Asset 等有狀態核心，則透過**精細的交易對分片 (Sharding)** 實現資源隔離與線性擴容。                                    | 修正後的擴展性描述。                |

---

## 3. SDK 設計與架構治理 (SDK Design & Governance)

**目標：** 展示在如此高效但複雜的架構下，如何透過標準化手段降低開發難度。

| 時間 | 畫面 (Visual) | 旁白腳本 (Audio) | 執行建議 |
| :--- | :--- | :--- | :--- |
| 2:35 | **[IDE 專案結構特寫]**<br>展開 `sdk` 目錄，快速掃過：<br>- `sdk-infra-kafka`<br>- `sdk-core`<br>- `sdk-auth`<br>背景淡出架構圖，聚焦代碼。 | 極致的效能往往伴隨著極高的複雜度。為了駕馭這套架構，我開發了一套標準化的 SDK，封裝了所有的底層細節。 | |
| 2:50 | **[程式碼展示：Spring Boot Starter]**<br>左側：業務邏輯代碼，乾淨簡單。<br>右側：SDK 內的 `KafkaAutoConfiguration`，顯示複雜的序列化、批量發送、冪等性檢查邏輯。 | 透過 Spring Boot Starter 機制，開發者只需要引入依賴，就能獲得經過調優的 Kafka 配置、Redis 連接池以及分佈式鎖實現。這確保了每一行業務代碼，都運行在最優的基礎設施之上。 | |
| 3:10 | **[視覺化效益：全鏈路追蹤]**<br>畫面顯示 Log：`[TraceId: xxxxx]`。<br>切換到 Zipkin/Grafana，顯示一條橫跨 Gateway -> Matching -> Risk 的完整調用鏈，時間軸精確到微秒。 | 同時，SDK 內建了全鏈路可觀測性。從用戶下單的那一刻起，唯一的 Trace ID 就貫穿了整個分佈式系統，讓我們能精確追蹤每一筆資金在微秒間的流動軌跡。 | |

---

## 4. 總結 (Wrap-up)

| 時間 | 畫面 (Visual) | 旁白腳本 (Audio) | 執行建議 |
| :--- | :--- | :--- | :--- |
| 3:30 | **[回到全景架構圖 + 關鍵字]**<br>文字依序浮現：<br>1. **CQRS & LMAX**<br>2. **In-Memory Matching**<br>3. **Double-Entry Bookkeeping** | Open Exchange Core 的架構哲學很簡單：透過 CQRS 分離讀寫，透過內存計算極大化效能，透過複式記帳確保資金安全。這是一個為速度與準確性而生的金融核心。 | |